[TOC]

## CAP 定理

CAP 定理(CAP theorem)，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition-tolerance）这三个特性最多只能满足两个，不能三者兼得（传说中的“三选二定理”）。

- C **一致性** ：分布式系统中的所有节点的数据是否在同一时刻保持一致。

- A  **可用性**：每一个请求都能够在**有限的时间内**返回**正常的响应结果（不能是系统错误）**。

- P **分区容错性（比较难理解）**：这里的分区指的是网络分区。分布式系统中部分节点**因为网络问题**导致不连通，就会形成网络分区（网络连通的分区和网络不连通的分区）。分区容错性的意思是出现网络分区时，系统仍然能够正常工作。

  如果一个数据只保存在一个节点上，一旦这个节点出现网络分区，系统就无法正常工作，这样的系统是没有分区容错性可言的。要保证容错性，就要将数据复制到多个节点上，那么即使出现网络分区，数据仍然能在其他分区中读取。然而把数据复制到多个节点，就会带来**一致性的问题**，就是多个节点上面的数据可能是不一致的。也会带来**可用性的问题**。

  如果要保证**一致性**，每次写操作就都要等待全部节点写成功，那么无法在有限的时间内返回响应结果，也就无法保证可用性。

  如果要保证**可用性**，那么就无法在有限的时间内使各个节点的数据保持一致，也就无法保证一致性，同时返回的响应结果可能也不是最新的。保证可用性就要允许部分节点的数据不是最新的。
  
  传统的单机架构就是CA，没有分区容错性。分布式系统只能满足CP或AP，无法满足CAP。

总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制同步的数据就越多，一致性就越难保证。为了保证一致性，复制同步所有节点数据所需要的时间就越长，可用性就会降低。



**注意：CAP定理的最新解释，不是所谓的3选2（不要被网上大多数文章误导了）**:

**当发生网络分区的时候，如果我们要继续服务，只能在强一致性和可用性之间选择其中一个（2选1，不是3选2）。也就是说当发生网络分区之后，P是前提，是必须要实现的，决定了P之后才有C和A的选择。**

大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在CAP理论诞生12年之后，CAP之父也在2012年重写了之前的论文。



参考：

[CAP和BASE理论](https://yq.aliyun.com/users/2tuvx2fgo5xxo)

[分布式系统的经典基础理论](https://blog.csdn.net/qq_34337272/article/details/80444032)

[分布式系统之CAP理论](https://www.cnblogs.com/hxsyl/p/4381980.html)



## BASE 理论

BASE 理论是对 CAP 定理的延伸，核心思想是即使无法做到强一致性（CAP的一致性就是强一致性），但可以采用适当的方式来使系统达到弱一致性，即最终一致性。

BASE 是 Basically Available（基本可用）、Soft state（软状态）和 Eventually consistent（最终一致性）三个短语的简写。

- BA 基本可用：分布式系统在出现故障时，允许损失部分可用性（响应时间增加、功能降级），但不是不可用。
- S 软状态：允许系统出现**中间状态**，而这个中间状态不会影响系统整体可用性。比如响应时间增加、功能降级。
- E 最终一致性：分布式系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一致的状态。

参考：[CAP和BASE理论](https://yq.aliyun.com/users/2tuvx2fgo5xxo)

## 分布式锁的实现方式

参考优秀博客：http://www.hollischuang.com/archives/1716

1. 基于数据库实现分布式锁 
2. 基于缓存实现分布式锁（Redis 使用 Redlock）
3. 基于 Zookeeper 实现分布式锁

最好使用 Zookeeper 实现分布式锁。

## 如何保证分布式 session 的一致性

由于 session 是保存在服务器（这里指的是应用服务器，即 tomcat、jetty、undertow 等）上，而在分布式环境下，每台服务器的 session 是不一样的，所以要保证 session 的一致性，可以采取以下措施：

- 使用 nginx ip hash 负载均衡策略，保证每个请求转发到固定的服务器
- 将 session 保存到 redis 缓存中（推荐）



## 什么是服务熔断？什么是服务降级？

服务雪崩：分布式系统中会有多个服务互相调用，如果服务提供者的不可用，会导致服务消费者的不可用，而服务消费者又会是另一个服务的提供者，这种不可用就会像滚雪球一样逐渐放大，进而引起整个系统不可用，这就叫服务雪崩。

服务熔断：是应对服务雪崩的一种保护机制，如果某个服务不可用或者响应时间过长，就会进行服务降级，直接返回预设的失败结果。

服务降级：当服务器压力剧增时，根据实际的业务情况和流量，对一些不重要的服务和页面有策略地不处理或者换种方式处理，从而释放服务器资源，保障核心业务正常运行。

参考：

[微服务架构—服务降级](https://blog.csdn.net/ityouknow/article/details/81230412)  

[springcloud(四)：熔断器Hystrix](http://www.ityouknow.com/springcloud/2017/05/16/springcloud-hystrix.html)



## Hystrix 的机制

当 Hystrix Command 请求后端服务失败数量超过一定比例(默认50%), 断路器会切换到开路状态(Open). 这时所有请求会直接失败而不会发送到后端服务. 断路器保持在开路状态一段时间后(默认5秒), 自动切换到半开路状态(HALF-OPEN). 这时会判断下一次请求的返回情况, 如果请求成功, 断路器切回闭路状态(CLOSED), 否则重新切换到开路状态(OPEN). Hystrix的断路器就像我们家庭电路中的保险丝, 一旦后端服务不可用, 断路器会直接切断请求链, 避免发送大量无效请求影响系统吞吐量, 并且断路器有自我检测并恢复的能力.

参考：[SpringCloud(四)：熔断器Hystrix](http://www.ityouknow.com/springcloud/2017/05/16/springcloud-hystrix.html)

## Hystrix 资源隔离

（1）线程池隔离模式：使用一个线程池来存储当前的请求，线程池对请求作处理，设置任务返回处理超时时间，堆积的请求堆积入线程池队列。这种方式需要为每个依赖的服务申请线程池，有一定的资源消耗，好处是可以应对突发流量（流量洪峰来临时，处理不完可将数据存储到线程池队里慢慢处理）
 （2）信号量隔离模式：使用一个原子计数器（或信号量）来记录当前有多少个线程在运行，请求来先判断计数器的数值，若超过设置的最大线程个数则丢弃改类型的新请求，若不超过则执行计数操作请求来计数器+1，请求返回计数器-1。这种方式是严格的控制线程且立即返回模式，无法应对突发流量（流量洪峰来临时，处理的线程超过数量，其他的请求会直接返回，不继续去请求依赖的服务）

参考：[hystrix原理](https://www.jianshu.com/p/e07661b9bae8)

## Hystrix 适用场景

Hystrix 限流、熔断、降级

核心业务不能使用熔断

## 如何使用 Hystrix ？

- 使用 @HystrixCommand 注解，指定 fallback 方法

```java
@HystrixCommand(fallbackMethod = "reliable")
public String readingList() {
    URI uri = URI.create("http://localhost:8090/recommended");

    return this.restTemplate.getForObject(uri, String.class);
}

public String reliable() {
    return "Cloud Native Java (O'Reilly)";
}
```

- 使用 @FeignClient 注解，指定 FallbackFactory 或 Fallback 接口实现类

```java
@FeignClient(value = "service-producer", fallbackFactory = HelloClient.HelloClientFallbackFactory.class)
```

代码地址：https://github.com/mondari/spring-cloud-learning/blob/master/eureka-client-consumer/src/main/java/com/mondari/HelloClient.java



## *限流算法

- **计算器**算法：限制一秒钟的能够通过的请求数

- **漏桶**算法：

  实现思路：准备一个队列，用来保存请求，另外通过一个线程池定期从队列中获取请求并执行，一次性可以获取多个并发执行。

- **令牌桶**：令牌工程匀速产生令牌并放到令牌桶中，然后客户端每次请求都要先从令牌桶中获取令牌，才能被后端服务处理。

  实现思路：准备一个队列，用来保存令牌，另外通过一个线程池定期生成令牌放到队列中，每来一个请求，就从队列中获取一个令牌，并继续执行。

参考：[spring cloud gateway 之限流篇](https://www.fangzhipeng.com/springcloud/2018/12/22/sc-f-gatway4.html)

## Feign 接口的原理



参考：https://blog.csdn.net/weixin_34194359/article/details/91420649

## Ribbon 负载均衡

## ZooKeeper ZAB 协议

ZAB 协议定义了 ZooKeeper 集群中节点的三种角色：

- Leader：提供读写服务，处理事务和非事务请求，是集群中事务请求的唯一处理者
- Follower：提供读服务，只能处理非事务请求，遇到事务请求则转发给 Leader，并参与投票
- Observer：同 Follower，但不参与投票

ZAB 协议定义了 ZooKeeper 集群中节点的四种状态：

- LOOKING，选举状态。
- LEADING，领导者状态。
- FOLLOWING，跟随者状态，参与投票。
- OBSERVING，观察者状态，不参与投票。

ZAB 协议从两个方面保证了分布式数据的一致性，一个是读写数据的流程，一个是 Leader 选举的流程。

## ZooKeeper 读写数据的流程

读数据流程：客户端发送请求给集群中的任一节点，不管该节点是 Leader 还是 Follower，直接读取节点中的数据即可

写数据流程：

1. 客户端发送写请求给集群中的某个节点，如果该节点不是 Leader，则转发写请求给 Leader，

2. Leader 会广播请求给集群中的所有节点

3. 节点收到广播后会进行写操作，如果写操作成功，就会通知 Leader

4. 只要集群中有超过一半的节点写操作成功，就认为数据写成功了，Leader 就会通知客户端连接的那个节点操作成功

   

## ZooKeeper Leader选举的流程

参考：

[【分布式】Zookeeper的Leader选举](https://www.cnblogs.com/leesf456/p/6107600.html)

[理解zookeeper选举机制](https://www.cnblogs.com/ASPNET2008/p/6421571.html)

[zookeeper集群搭建及选举模式](https://blog.csdn.net/why15732625998/article/details/80867151)

[漫画：什么是ZooKeeper？](https://juejin.im/post/5b037d5c518825426e024473)


​    

当集群中的一台服务器出现以下两种情况之一时，就会进入 Leader 选举状态：

- 服务器启动时进行初始化
- 服务器运行时无法和 Leader 保持连接

Leader 选举的流程如下：

1. 发起投票：每个节点投票给自己，并发送投票信息给其它节点，投票信息主要包括 SID、ZXID，另外还有逻辑时钟和选举状态 LOOKING。（SID 即 myid，也就是服务器ID；ZXID 是事务ID，值越大表示数据越新；逻辑时钟也叫投票的轮数，每进行一轮投票就会加一；选举状态也就是四种节点的状态）
2. 接收投票：每个节点接收来自其它节点的投票，判断是否是本轮的投票，是否是来自 LOOKING 状态节点的投票
3. 处理投票：每接收到一个节点的投票，就会将其和自己的投票进行比较，优先看选票中谁的 ZXID 最大，然后更新自己的投票为最大的 ZXID 的投票，并发送投票信息给其它节点。如果 ZXID 相同，则看谁的 SID 最大。
4. 统计投票：每进行一轮投票，都会统计投票信息，投票数超过一半的节点就会成为 Leader 节点
5. 变更状态：一旦确定了 Leader，每个节点就会更新自己的状态，如果是 Follower，就变更为FOLLOWING，如果是 Leader，就变更为LEADING。



如果**外部投票的选举轮次大于内部投票**，就会立即更新自己的选举轮次(logicalclock)，并且清空所有已经收到的投票，然后使用初始化的投票来进行PK以确定是否变更内部投票。最终再将内部投票发送出去。

## ZooKeeper 的使用场景

1. 服务注册和服务发现
2. 配置中心
3. 分布式锁

参考：https://spring.io/projects/spring-cloud-zookeeper

